{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"functions used to construct different architectures  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#tf.compat.v1.flags\n",
    "FLAGS = tf.compat.v1.app.flags.FLAGS\n",
    "\n",
    "def int_shape(x):\n",
    "  return list(map(int, x.shape))\n",
    "\n",
    "def concat_elu(x):\n",
    "    \"\"\" like concatenated ReLU (http://arxiv.org/abs/1603.05201), but then with ELU \"\"\"\n",
    "    axis = len(x.get_shape())-1\n",
    "    return tf.nn.elu(tf.concat(values=[x, -x], axis=axis))\n",
    "\n",
    "def set_nonlinearity(name):\n",
    "  if name == 'concat_elu':\n",
    "    return concat_elu\n",
    "  elif name == 'elu':\n",
    "    return tf.nn.elu\n",
    "  elif name == 'concat_relu':\n",
    "    return tf.nn.crelu\n",
    "  elif name == 'relu':\n",
    "    return tf.nn.relu\n",
    "  else:\n",
    "    raise('nonlinearity ' + name + ' is not supported')\n",
    "\n",
    "def _activation_summary(x):\n",
    "  \"\"\"Helper to create summaries for activations.\n",
    "  Creates a summary that provides a histogram of activations.\n",
    "  Creates a summary that measure the sparsity of activations.\n",
    "  Args:\n",
    "    x: Tensor\n",
    "  Returns:\n",
    "    nothing\n",
    "  \"\"\"\n",
    "  tensor_name = x.op.name\n",
    "  tf.summary.histogram(tensor_name + '/activations', x)\n",
    "  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "def _variable(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  # getting rid of stddev for xavier ## testing this for faster convergence\n",
    "  var = tf.get_variable(name, shape, initializer=initializer)\n",
    "  return var\n",
    "\n",
    "def conv_layer(inputs, kernel_size, stride, num_features, idx, nonlinearity=None):\n",
    "  with tf.variable_scope('{0}_conv'.format(idx)) as scope:\n",
    "    input_channels = int(inputs.get_shape()[3])\n",
    "\n",
    "    weights = _variable('weights', shape=[kernel_size,kernel_size,input_channels,num_features],initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = _variable('biases',[num_features],initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "    conv = tf.nn.conv2d(inputs, weights, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    conv_biased = tf.nn.bias_add(conv, biases)\n",
    "    if nonlinearity is not None:\n",
    "      conv_biased = nonlinearity(conv_biased)\n",
    "    return conv_biased\n",
    "\n",
    "def transpose_conv_layer(inputs, kernel_size, stride, num_features, idx, nonlinearity=None):\n",
    "  with tf.variable_scope('{0}_trans_conv'.format(idx)) as scope:\n",
    "    input_channels = int(inputs.get_shape()[3])\n",
    "    \n",
    "    weights = _variable('weights', shape=[kernel_size,kernel_size,num_features,input_channels],initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = _variable('biases',[num_features],initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    output_shape = tf.stack([tf.shape(inputs)[0], tf.shape(inputs)[1]*stride, tf.shape(inputs)[2]*stride, num_features]) \n",
    "    conv = tf.nn.conv2d_transpose(inputs, weights, output_shape, strides=[1,stride,stride,1], padding='SAME')\n",
    "    conv_biased = tf.nn.bias_add(conv, biases)\n",
    "    if nonlinearity is not None:\n",
    "      conv_biased = nonlinearity(conv_biased)\n",
    "\n",
    "    #reshape\n",
    "    shape = int_shape(inputs)\n",
    "    conv_biased = tf.reshape(conv_biased, [shape[0], shape[1]*stride, shape[2]*stride, num_features])\n",
    "\n",
    "    return conv_biased\n",
    "\n",
    "def fc_layer(inputs, hiddens, idx, nonlinearity=None, flat = False):\n",
    "  with tf.variable_scope('{0}_fc'.format(idx)) as scope:\n",
    "    input_shape = inputs.get_shape().as_list()\n",
    "    if flat:\n",
    "      dim = input_shape[1]*input_shape[2]*input_shape[3]\n",
    "      inputs_processed = tf.reshape(inputs, [-1,dim])\n",
    "    else:\n",
    "      dim = input_shape[1]\n",
    "      inputs_processed = inputs\n",
    "    \n",
    "    weights = _variable('weights', shape=[dim,hiddens],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = _variable('biases', [hiddens], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    output_biased = tf.add(tf.matmul(inputs_processed,weights),biases,name=str(idx)+'_fc')\n",
    "    if nonlinearity is not None:\n",
    "      output_biased = nonlinearity(ouput_biased)\n",
    "    return output_biased\n",
    "\n",
    "def nin(x, num_units, idx):\n",
    "    \"\"\" a network in network layer (1x1 CONV) \"\"\"\n",
    "    s = int_shape(x)\n",
    "    x = tf.reshape(x, [np.prod(s[:-1]),s[-1]])\n",
    "    x = fc_layer(x, num_units, idx)\n",
    "    return tf.reshape(x, s[:-1]+[num_units])\n",
    "\n",
    "def _phase_shift(I, r):\n",
    "  bsize, a, b, c = I.get_shape().as_list()\n",
    "  bsize = tf.shape(I)[0] # Handling Dimension(None) type for undefined batch dim\n",
    "  X = tf.reshape(I, (bsize, a, b, r, r))\n",
    "  X = tf.transpose(X, (0, 1, 2, 4, 3))  # bsize, a, b, 1, 1\n",
    "  X = tf.split(axis=1, num_or_size_splits=a, value=X)  # a, [bsize, b, r, r]\n",
    "  X = tf.concat(axis=2, values=[tf.squeeze(x) for x in X])  # bsize, b, a*r, r\n",
    "  X = tf.split(axis=1, num_or_size_splits=b, value=X)  # b, [bsize, a*r, r]\n",
    "  X = tf.concat(axis=2, values=[tf.squeeze(x) for x in X])  # bsize, a*r, b*r\n",
    "  return tf.reshape(X, (bsize, a*r, b*r, 1))\n",
    "\n",
    "def PS(X, r, depth):\n",
    "  Xc = tf.split(axis=3, num_or_size_splits=depth, value=X)\n",
    "  X = tf.concat(axis=3, values=[_phase_shift(x, r) for x in Xc])\n",
    "  return X\n",
    "\n",
    "def res_block(x, a=None, filter_size=16, nonlinearity=concat_elu, keep_p=1.0, stride=1, gated=False, name=\"resnet\"):\n",
    "  orig_x = x\n",
    "  orig_x_int_shape = int_shape(x)\n",
    "  if orig_x_int_shape[3] == 1:\n",
    "    x_1 = conv_layer(x, 3, stride, filter_size, name + '_conv_1')\n",
    "  else:\n",
    "    x_1 = conv_layer(nonlinearity(x), 3, stride, filter_size, name + '_conv_1')\n",
    "  if a is not None:\n",
    "    shape_a = int_shape(a) \n",
    "    shape_x_1 = int_shape(x_1)\n",
    "    a = tf.pad(\n",
    "      a, [[0, 0], [0, shape_x_1[1]-shape_a[1]], [0, shape_x_1[2]-shape_a[2]],\n",
    "      [0, 0]])\n",
    "    x_1 += nin(nonlinearity(a), filter_size, name + '_nin')\n",
    "  x_1 = nonlinearity(x_1)\n",
    "  if keep_p < 1.0:\n",
    "    x_1 = tf.nn.dropout(x_1, keep_prob=keep_p)\n",
    "  if not gated:\n",
    "    x_2 = conv_layer(x_1, 3, 1, filter_size, name + '_conv_2')\n",
    "  else:\n",
    "    x_2 = conv_layer(x_1, 3, 1, filter_size*2, name + '_conv_2')\n",
    "    x_2_1, x_2_2 = tf.split(axis=3,num_or_size_splits=2,value=x_2)\n",
    "    x_2 = x_2_1 * tf.nn.sigmoid(x_2_2)\n",
    "\n",
    "  if int(orig_x.get_shape()[2]) > int(x_2.get_shape()[2]):\n",
    "    assert(int(orig_x.get_shape()[2]) == 2*int(x_2.get_shape()[2]), \"res net block only supports stirde 2\")\n",
    "    orig_x = tf.nn.avg_pool(orig_x, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "\n",
    "  # pad it\n",
    "  out_filter = filter_size\n",
    "  in_filter = int(orig_x.get_shape()[3])\n",
    "  if out_filter != in_filter:\n",
    "    orig_x = tf.pad(\n",
    "        orig_x, [[0, 0], [0, 0], [0, 0],\n",
    "        [(out_filter-in_filter), 0]])\n",
    "\n",
    "  return orig_x + x_2\n",
    "\n",
    "def conv_res(inputs, nr_res_blocks=1, keep_prob=1.0, nonlinearity_name='concat_elu', gated=True):\n",
    "  \"\"\"Builds conv part of net.\n",
    "  Args:\n",
    "    inputs: input images\n",
    "    keep_prob: dropout layer\n",
    "  \"\"\"\n",
    "  nonlinearity = set_nonlinearity(nonlinearity_name)\n",
    "  filter_size = 8\n",
    "  # store for as\n",
    "  a = []\n",
    "  # res_1\n",
    "  x = inputs\n",
    "  for i in range(nr_res_blocks):\n",
    "    x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_1_\" + str(i))\n",
    "  # res_2\n",
    "  a.append(x)\n",
    "  filter_size = 2 * filter_size\n",
    "  x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, stride=2, gated=gated, name=\"resnet_2_downsample\")\n",
    "  for i in range(nr_res_blocks):\n",
    "    x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_2_\" + str(i))\n",
    "  # res_3\n",
    "  a.append(x)\n",
    "  filter_size = 2 * filter_size\n",
    "  x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, stride=2, gated=gated, name=\"resnet_3_downsample\")\n",
    "  for i in range(nr_res_blocks):\n",
    "    x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_3_\" + str(i))\n",
    "  # res_4\n",
    "  a.append(x)\n",
    "  filter_size = 2 * filter_size\n",
    "  x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, stride=2, gated=gated, name=\"resnet_4_downsample\")\n",
    "  for i in range(nr_res_blocks):\n",
    "    x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_4_\" + str(i))\n",
    "  # res_4\n",
    "  a.append(x)\n",
    "  filter_size = 2 * filter_size\n",
    "  x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, stride=2, gated=gated, name=\"resnet_5_downsample\")\n",
    "  for i in range(nr_res_blocks):\n",
    "    x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_5_\" + str(i))\n",
    "  # res_up_1\n",
    "  filter_size = int(filter_size /2)\n",
    "  x = transpose_conv_layer(x, 3, 2, filter_size, \"up_conv_1\")\n",
    "  #x = PS(x,2,512)\n",
    "  for i in range(nr_res_blocks):\n",
    "    if i == 0:\n",
    "      x = res_block(x, a=a[-1], filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_1_\" + str(i))\n",
    "    else:\n",
    "      x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_1_\" + str(i))\n",
    "  # res_up_1\n",
    "  filter_size = int(filter_size /2)\n",
    "  x = transpose_conv_layer(x, 3, 2, filter_size, \"up_conv_2\")\n",
    "  #x = PS(x,2,512)\n",
    "  for i in range(nr_res_blocks):\n",
    "    if i == 0:\n",
    "      x = res_block(x, a=a[-2], filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_2_\" + str(i))\n",
    "    else:\n",
    "      x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_2_\" + str(i))\n",
    "\n",
    "  filter_size = int(filter_size /2)\n",
    "  x = transpose_conv_layer(x, 3, 2, filter_size, \"up_conv_3\")\n",
    "  #x = PS(x,2,512)\n",
    "  for i in range(nr_res_blocks):\n",
    "    if i == 0:\n",
    "      x = res_block(x, a=a[-3], filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_3_\" + str(i))\n",
    "    else:\n",
    "      x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_3_\" + str(i))\n",
    " \n",
    "  filter_size = int(filter_size /2)\n",
    "  x = transpose_conv_layer(x, 3, 2, filter_size, \"up_conv_4\")\n",
    "  #x = PS(x,2,512)\n",
    "  for i in range(nr_res_blocks):\n",
    "    if i == 0:\n",
    "      x = res_block(x, a=a[-4], filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_4_\" + str(i))\n",
    "    else:\n",
    "      x = res_block(x, filter_size=filter_size, nonlinearity=nonlinearity, keep_p=keep_prob, gated=gated, name=\"resnet_up_4_\" + str(i))\n",
    "  \n",
    "  x = conv_layer(x, 3, 1, 2, \"last_conv\")\n",
    "  x = tf.nn.tanh(x) \n",
    "\n",
    "  tf.summary.image('sflow_p_x', x[:,:,:,1:2])\n",
    "  tf.summary.image('sflow_p_v', x[:,:,:,0:1])\n",
    "\n",
    "  return x\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
